# Task T-1.3.2: Retry Handler

## Meta
| Field | Value |
|-------|-------|
| Task ID | T-1.3.2 |
| Milestone | M1: Foundation |
| Module | M1-MOD3: Resilience Layer |
| Priority | High |
| Effort | 3h |
| Estimated Tokens | 18K |
| Recommended Model | Sonnet |
| Dependencies | T-1.2.2 |
| Parallel Group | PG-3 |
| AC Mapping | AC-013 |

## Objective
Implement a retry handler that automatically retries failed HTTP requests with exponential backoff and jitter. The handler detects retryable errors (HTTP 429, 503, network errors) and respects the `Retry-After` header when present. Non-retryable errors are propagated immediately without retry.

## Acceptance Criteria
- [ ] Configurable `max_retries` (default: 3 from config) (AC-013)
- [ ] Exponential backoff: delay doubles with each attempt (e.g., 1s, 2s, 4s)
- [ ] Jitter added to backoff to prevent thundering herd (randomized within a range)
- [ ] Retryable error detection: HTTP 429 (Rate Limited), HTTP 503 (Service Unavailable), network errors (connection refused, timeout, DNS failure)
- [ ] Non-retryable errors (400, 401, 403, 404, 500) are thrown immediately without retry
- [ ] `Retry-After` header is respected when present — uses the header value instead of calculated backoff
- [ ] After all retries are exhausted, the last error is thrown
- [ ] The retry handler wraps an async function (higher-order function pattern)
- [ ] Unit tests verify: retry on 429, retry on 503, retry on network error, no retry on 400, Retry-After header respect, max retries exhaustion

## Technical Notes

### File Location
```
src/resilience/retry.ts
```

### Retry Handler Design
Use a higher-order function pattern that wraps an async operation:

```typescript
export interface RetryConfig {
  maxRetries: number;
  baseDelayMs: number;      // Default: 1000
  maxDelayMs: number;        // Default: 30000 (cap for backoff)
}

export async function withRetry<T>(
  operation: () => Promise<T>,
  config: RetryConfig,
): Promise<T> {
  let lastError: Error;

  for (let attempt = 0; attempt <= config.maxRetries; attempt++) {
    try {
      return await operation();
    } catch (error) {
      lastError = error as Error;

      if (attempt === config.maxRetries || !isRetryable(error)) {
        throw lastError;
      }

      const delay = calculateDelay(attempt, config, error);
      await sleep(delay);
    }
  }

  throw lastError!;
}
```

### Retryable Error Detection
```typescript
export function isRetryable(error: unknown): boolean {
  if (error instanceof ServiceNowHttpError) {
    return error.statusCode === 429 || error.statusCode === 503;
  }
  // Network errors: TypeError from fetch (connection refused, DNS), AbortError (timeout)
  if (error instanceof TypeError) return true;  // fetch network errors
  if (error instanceof DOMException && error.name === "AbortError") return true;  // timeout
  return false;
}
```

### Exponential Backoff with Jitter
```typescript
function calculateDelay(
  attempt: number,
  config: RetryConfig,
  error: unknown,
): number {
  // Respect Retry-After header if present
  const retryAfter = getRetryAfterMs(error);
  if (retryAfter !== null) {
    return retryAfter;
  }

  // Exponential backoff: baseDelay * 2^attempt
  const exponentialDelay = config.baseDelayMs * Math.pow(2, attempt);

  // Cap at maxDelay
  const cappedDelay = Math.min(exponentialDelay, config.maxDelayMs);

  // Full jitter: random value between 0 and cappedDelay
  return Math.random() * cappedDelay;
}
```

### Retry-After Header Support
```typescript
function getRetryAfterMs(error: unknown): number | null {
  if (!(error instanceof ServiceNowHttpError)) return null;

  // Retry-After can be seconds (integer) or HTTP-date
  const retryAfter = error.retryAfterHeader;
  if (!retryAfter) return null;

  const seconds = parseInt(retryAfter, 10);
  if (!isNaN(seconds)) {
    return seconds * 1000;
  }

  // Handle HTTP-date format
  const date = new Date(retryAfter);
  if (!isNaN(date.getTime())) {
    return Math.max(0, date.getTime() - Date.now());
  }

  return null;
}
```

**Note**: The `ServiceNowHttpError` class (from T-1.2.2) may need to be extended to capture the `Retry-After` response header. Coordinate with T-1.2.2 to ensure the header is preserved.

### Integration Point
The retry handler wraps the HTTP client's request method:
```
Rate Limiter -> withRetry(() => client.request(...)) -> Circuit Breaker
```

### Key Decisions
- **Full jitter** (not equal jitter or decorrelated jitter) — simplest approach, effective at preventing thundering herd. See AWS Architecture Blog's analysis of jitter strategies.
- **Max delay cap**: Prevents absurdly long waits on high retry counts. Default 30 seconds is appropriate for interactive MCP tool calls.
- **HTTP 500 is NOT retryable**: ServiceNow 500 errors are typically application errors (bad query, internal logic), not transient. Only 429 and 503 are retryable.
- **Network errors are retryable**: Connection refused, DNS failure, and timeouts are likely transient in enterprise environments.
- **Higher-order function over class**: Simpler API, easier to compose, no state management needed.

### Testing Strategy
- **Retry on 429**: Mock operation to fail with 429 twice then succeed; verify 3 total calls
- **Retry on 503**: Similar to 429 test
- **Retry on network error**: Mock `TypeError` from fetch; verify retry
- **No retry on 400/401/403/404/500**: Mock operation to fail with these codes; verify single call
- **Retry-After header**: Mock 429 with `Retry-After: 2`; verify delay is ~2000ms (use fake timers)
- **Max retries exhaustion**: Mock operation to always fail with 429; verify exactly `maxRetries + 1` calls, then error is thrown
- **Jitter**: Verify delay is randomized (statistical test or mock `Math.random`)

## Token Estimation Rationale
| Component | Tokens |
|-----------|--------|
| Retry handler function (retry.ts) | 5K |
| Retryable error detection logic | 2K |
| Backoff calculation with jitter | 2K |
| Retry-After header parsing | 2K |
| Unit tests (all scenarios) | 5K |
| Debugging buffer (10%) | 2K |
| **Total** | **18K** |
